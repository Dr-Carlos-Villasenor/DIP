{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIP_039.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdNN/uO857CxTy9Dps7IRi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/DIP/blob/main/DIP_039.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis y Procesameinto de Imágenes\n",
        "## Dr. Carlos Villaseñor\n",
        "### DIP_039 Red Neuronal Densa (Perceptrón multicapa)"
      ],
      "metadata": {
        "id": "5kyPfPdN_MQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4dmX5Kp_HDe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multilayer Perceptron\n",
        "Dr. Carlos Villaseñor\n",
        "\"\"\"\n",
        "\n",
        "# Packages\n",
        "import numpy as np\n",
        "\n",
        "def linear(z, derivative=False):\n",
        "    a = z\n",
        "    if derivative:\n",
        "        da = 1\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def sigmoid(z, derivative=False):\n",
        "    a = 1/(1+np.exp(-z))\n",
        "    if derivative:\n",
        "        da = a * (1 - a)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def tanh(z, derivative=False):\n",
        "    a = np.tanh(z)\n",
        "    if derivative:\n",
        "        da = (1 - a) * (1 + a)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "\n",
        "def relu(z, derivative=False):\n",
        "    a = z * (z >= 0)\n",
        "    if derivative:\n",
        "        da = np.array(z >= 0, dtype=float)\n",
        "        return a, da\n",
        "    return a\n",
        "\n",
        "class MLP:\n",
        "\n",
        "    def __init__(self, layers_dims, \n",
        "                 hidden_activation=relu,\n",
        "                 output_activation=sigmoid,\n",
        "                 learning_rate=0.1):\n",
        "\n",
        "        # Instance Attributes\n",
        "        self.L = len(layers_dims) - 1\n",
        "        self.w = [None] * (self.L + 1)\n",
        "        self.b = [None] * (self.L + 1)\n",
        "        self.f = [None] * (self.L + 1)\n",
        "        self.layers = layers_dims\n",
        "        self.eta = learning_rate\n",
        "\n",
        "        # Initialize weights\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.w[l] = -1 + 2 * np.random.rand(layers_dims[l], layers_dims[l-1])\n",
        "            self.b[l] = -1 + 2 * np.random.rand(layers_dims[l], 1)\n",
        "            \n",
        "            if l == self.L:\n",
        "                self.f[l] = output_activation\n",
        "            else:\n",
        "                self.f[l] = hidden_activation\n",
        "                \n",
        "\n",
        "    def predict(self, X):\n",
        "        a = np.asanyarray(X)\n",
        "        for l in range(1, self.L + 1):\n",
        "            z = np.dot(self.w[l], a) + self.b[l]\n",
        "            a = self.f[l](z)\n",
        "        return a\n",
        "\n",
        "    def fit(self, X, Y, epochs=100):\n",
        "\n",
        "        # Number of samples\n",
        "        P = X.shape[1]\n",
        "\n",
        "        #Gradient Descent\n",
        "        for _ in range(epochs):\n",
        "            for p in range(P):\n",
        "\n",
        "                # Initialize activations and their derivatives\n",
        "                a = [None] * (self.L + 1)\n",
        "                da = [None] * (self.L + 1)\n",
        "                lg = [None] * (self.L + 1)\n",
        "                \n",
        "                # Propagation\n",
        "                a[0] = X[:, p].reshape(self.layers[0], 1)\n",
        "                for l in range(1, self.L + 1):\n",
        "                    z = np.dot(self.w[l], a[l-1]) + self.b[l]\n",
        "                    a[l], da[l] = self.f[l](z, derivative=True)\n",
        "\n",
        "                # Backpropagation\n",
        "                for l in range(self.L, 0, -1):\n",
        "                    # Calculate local gradient (lg)\n",
        "                    if l == self.L:\n",
        "                        lg[l] = (Y[:, p] - a[l]) * da[l]\n",
        "                    else:\n",
        "                        lg[l] = np.dot(self.w[l + 1].T, lg[l+1]) * da[l]\n",
        "                        \n",
        "                # Update parameters\n",
        "                for l in range(1, self.L + 1):\n",
        "                    self.w[l] += self.eta * np.dot(lg[l], a[l - 1].T)\n",
        "                    self.b[l] += self.eta * lg[l]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para dibujar problemas 2D"
      ],
      "metadata": {
        "id": "V0Lxa_C1_cti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def MLP_binary_classification_2d(X,Y,net):\n",
        "    plt.figure()\n",
        "    for i in range(X.shape[1]):\n",
        "        if Y[0,i]==0:\n",
        "            plt.plot(X[0,i], X[1,i], '.r')\n",
        "        else:\n",
        "            plt.plot(X[0,i], X[1,i], '.b')\n",
        "    xmin, ymin=np.min(X[0,:])-0.5, np.min(X[1,:])-0.5\n",
        "    xmax, ymax=np.max(X[0,:])+0.5, np.max(X[1,:])+0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(xmin,xmax,100), \n",
        "                         np.linspace(ymin,ymax,100))\n",
        "    data = [xx.ravel(), yy.ravel()]\n",
        "    zz = net.predict(data)\n",
        "    zz = zz.reshape(xx.shape)\n",
        "    plt.contourf(xx,yy,zz, alpha=0.8, \n",
        "                 cmap=plt.cm.RdBu)\n",
        "    plt.xlim([xmin,xmax])\n",
        "    plt.ylim([ymin,ymax])\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8u8xtmvt_cNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear el dataset para el problema XOR"
      ],
      "metadata": {
        "id": "8SBxqh2O_kbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0, 0, 1, 1],\n",
        "              [0, 1, 0, 1]])\n",
        "Y = np.array([[1, 0, 0, 1]]) "
      ],
      "metadata": {
        "id": "DLfEqVyh_nEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibujemos primeramente que nos da la red neuronal sin entrenar"
      ],
      "metadata": {
        "id": "Qbld--Kl_qQq"
      }
    }
  ]
}